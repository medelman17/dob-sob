{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🗽 NYC DOB Fraud Detection - Data Exploration\n",
    "\n",
    "This notebook provides an interactive environment for exploring the 25GB NYC Department of Buildings dataset and developing fraud detection algorithms.\n",
    "\n",
    "## What's Available\n",
    "- **94 Datasets**: Complete NYC DOB data (violations, permits, complaints, etc.)\n",
    "- **25GB of Data**: Comprehensive fraud detection dataset\n",
    "- **Neo4j Database**: Graph analysis capabilities\n",
    "- **ML Libraries**: Scikit-learn, NetworkX, Pandas, Polars\n",
    "\n",
    "## Quick Start\n",
    "1. Run the setup cell below\n",
    "2. Load a dataset for exploration\n",
    "3. Develop fraud detection patterns\n",
    "4. Test community detection algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🗽 NYC DOB Fraud Detection Environment Ready!\")\n",
    "print(f\"📊 Libraries loaded: pandas {pd.__version__}, polars {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Dataset Discovery\n",
    "\n",
    "Let's explore what datasets are available and their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available datasets\n",
    "data_dir = Path(\"/app/data/raw\")\n",
    "datasets = {}\n",
    "\n",
    "for dataset_dir in data_dir.iterdir():\n",
    "    if dataset_dir.is_dir():\n",
    "        csv_files = list(dataset_dir.glob(\"*.csv\"))\n",
    "        if csv_files:\n",
    "            latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)\n",
    "            size_mb = latest_csv.stat().st_size / (1024 * 1024)\n",
    "            datasets[dataset_dir.name] = {\n",
    "                'path': latest_csv,\n",
    "                'size_mb': round(size_mb, 2),\n",
    "                'modified': datetime.fromtimestamp(latest_csv.stat().st_mtime)\n",
    "            }\n",
    "\n",
    "# Display datasets sorted by size\n",
    "datasets_df = pd.DataFrame([\n",
    "    {'Dataset': name, 'Size (MB)': info['size_mb'], 'Modified': info['modified']}\n",
    "    for name, info in datasets.items()\n",
    "]).sort_values('Size (MB)', ascending=False)\n",
    "\n",
    "print(f\"📊 Found {len(datasets)} datasets totaling {datasets_df['Size (MB)'].sum():.1f} MB\")\n",
    "print(\"\\n🔝 Top 10 Largest Datasets:\")\n",
    "display(datasets_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Load and Explore a Dataset\n",
    "\n",
    "Let's start with one of the key fraud detection datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a key dataset (you can change this)\n",
    "DATASET_NAME = \"dob_violations\"  # Change this to explore different datasets\n",
    "SAMPLE_SIZE = 10000  # Adjust for performance\n",
    "\n",
    "if DATASET_NAME in datasets:\n",
    "    dataset_path = datasets[DATASET_NAME]['path']\n",
    "    \n",
    "    print(f\"📂 Loading {DATASET_NAME} ({datasets[DATASET_NAME]['size_mb']} MB)\")\n",
    "    print(f\"🔄 Using sample of {SAMPLE_SIZE:,} rows for quick exploration\")\n",
    "    \n",
    "    # Load sample for quick exploration\n",
    "    df = pd.read_csv(dataset_path, nrows=SAMPLE_SIZE, low_memory=False)\n",
    "    \n",
    "    print(f\"✅ Loaded {len(df):,} rows × {len(df.columns)} columns\")\n",
    "    print(f\"💾 Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Basic info\n",
    "    display(df.head())\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ Dataset '{DATASET_NAME}' not found\")\n",
    "    print(f\"Available datasets: {list(datasets.keys())[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Quick Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"📋 Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()} total\")\n",
    "\n",
    "# Show data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "missing_data = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_data.plot(kind='bar')\n",
    "    plt.title('Missing Data by Column')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Missing Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✅ No missing data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Fraud Detection Analysis\n",
    "\n",
    "Look for patterns that might indicate fraudulent activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key columns for fraud detection\n",
    "fraud_columns = []\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(keyword in col_lower for keyword in \n",
    "           ['bin', 'contractor', 'owner', 'violation', 'permit', 'license', 'address']):\n",
    "        fraud_columns.append(col)\n",
    "\n",
    "print(f\"🔍 Key columns for fraud detection: {fraud_columns}\")\n",
    "\n",
    "# Show sample of fraud-relevant data\n",
    "if fraud_columns:\n",
    "    display(df[fraud_columns].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for potential patterns\n",
    "# Example: Frequency analysis of key entities\n",
    "\n",
    "if 'bin' in [col.lower() for col in df.columns]:\n",
    "    bin_col = [col for col in df.columns if col.lower() == 'bin'][0]\n",
    "    \n",
    "    # Properties with multiple violations (potential red flag)\n",
    "    bin_counts = df[bin_col].value_counts()\n",
    "    high_violation_properties = bin_counts[bin_counts > 5]  # Properties with >5 violations\n",
    "    \n",
    "    print(f\"🚨 Properties with >5 violations: {len(high_violation_properties)}\")\n",
    "    print(f\"🏆 Top violators:\")\n",
    "    display(high_violation_properties.head(10))\n",
    "    \n",
    "    # Visualize violation distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bin_counts.head(20).plot(kind='bar')\n",
    "    plt.title('Top 20 Properties by Violation Count')\n",
    "    plt.xlabel('Building ID (BIN)')\n",
    "    plt.ylabel('Violation Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🕸️ Network Analysis for Fraud Detection\n",
    "\n",
    "Create graphs to identify suspicious relationships between entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import network analysis libraries\n",
    "import networkx as nx\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"🕸️ Network analysis libraries loaded\")\n",
    "print(\"📈 Ready for community detection algorithms:\")\n",
    "print(\"   • Louvain community detection\")\n",
    "print(\"   • Label propagation\")\n",
    "print(\"   • DBSCAN clustering\")\n",
    "print(\"   • Graph-based fraud pattern discovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a simple network from violation data\n",
    "# This connects properties (BIN) to violation types\n",
    "\n",
    "if 'bin' in [col.lower() for col in df.columns] and len(df) > 0:\n",
    "    # Find BIN and violation type columns\n",
    "    bin_col = [col for col in df.columns if col.lower() == 'bin'][0]\n",
    "    \n",
    "    # Look for violation type column\n",
    "    violation_cols = [col for col in df.columns if 'violation' in col.lower() or 'class' in col.lower()]\n",
    "    \n",
    "    if violation_cols:\n",
    "        violation_col = violation_cols[0]\n",
    "        \n",
    "        # Create a bipartite graph: Properties <-> Violation Types\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add edges between properties and violation types\n",
    "        for _, row in df.dropna(subset=[bin_col, violation_col]).head(1000).iterrows():  # Limit for performance\n",
    "            property_id = f\"PROP_{row[bin_col]}\"\n",
    "            violation_type = f\"VIOL_{row[violation_col]}\"\n",
    "            G.add_edge(property_id, violation_type)\n",
    "        \n",
    "        print(f\"🕸️ Created network with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "        print(f\"📊 Network density: {nx.density(G):.4f}\")\n",
    "        \n",
    "        # Find properties with most violation types (potential fraud indicators)\n",
    "        property_nodes = [n for n in G.nodes() if n.startswith('PROP_')]\n",
    "        property_degrees = [(node, G.degree(node)) for node in property_nodes]\n",
    "        property_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\n🚨 Properties with most violation types (potential fraud indicators):\")\n",
    "        for prop, degree in property_degrees[:10]:\n",
    "            print(f\"   {prop}: {degree} different violation types\")\n",
    "    else:\n",
    "        print(\"❌ No violation type column found for network analysis\")\n",
    "else:\n",
    "    print(\"❌ BIN column not found for network analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Connect to Neo4j Database\n",
    "\n",
    "For advanced graph analysis and persistent storage of relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j database\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    \n",
    "    # Connection details (using Docker service names)\n",
    "    NEO4J_URI = \"bolt://neo4j:7687\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD = \"password\"  # Change this to your actual password\n",
    "    \n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    \n",
    "    # Test connection\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"RETURN 'Connected to Neo4j!' as message\")\n",
    "        print(result.single()[\"message\"])\n",
    "        \n",
    "        # Show database stats\n",
    "        result = session.run(\"MATCH (n) RETURN count(n) as node_count\")\n",
    "        node_count = result.single()[\"node_count\"]\n",
    "        \n",
    "        result = session.run(\"MATCH ()-[r]->() RETURN count(r) as rel_count\")\n",
    "        rel_count = result.single()[\"rel_count\"]\n",
    "        \n",
    "        print(f\"📊 Neo4j Database: {node_count:,} nodes, {rel_count:,} relationships\")\n",
    "    \n",
    "    print(\"✅ Neo4j connection successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Neo4j connection failed: {e}\")\n",
    "    print(\"💡 Make sure Neo4j is running and credentials are correct\")\n",
    "    print(\"   You can access Neo4j browser at: http://localhost:37474\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Community Detection Example\n",
    "\n",
    "Detect communities in the violation network to find suspicious clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run community detection on the network we created\n",
    "if 'G' in locals() and G.number_of_nodes() > 0:\n",
    "    \n",
    "    # Use NetworkX's built-in community detection\n",
    "    try:\n",
    "        from networkx.algorithms import community\n",
    "        \n",
    "        # Louvain community detection\n",
    "        communities = community.louvain_communities(G, seed=42)\n",
    "        \n",
    "        print(f\"🏘️ Found {len(communities)} communities\")\n",
    "        \n",
    "        # Analyze communities\n",
    "        community_info = []\n",
    "        for i, comm in enumerate(communities):\n",
    "            properties = [n for n in comm if n.startswith('PROP_')]\n",
    "            violations = [n for n in comm if n.startswith('VIOL_')]\n",
    "            \n",
    "            community_info.append({\n",
    "                'Community': i,\n",
    "                'Size': len(comm),\n",
    "                'Properties': len(properties),\n",
    "                'Violation_Types': len(violations)\n",
    "            })\n",
    "        \n",
    "        comm_df = pd.DataFrame(community_info).sort_values('Size', ascending=False)\n",
    "        \n",
    "        print(\"\\n📊 Community Analysis:\")\n",
    "        display(comm_df.head(10))\n",
    "        \n",
    "        # Look for suspicious communities (many properties, few violation types)\n",
    "        suspicious = comm_df[\n",
    "            (comm_df['Properties'] > 2) & \n",
    "            (comm_df['Violation_Types'] <= 2)\n",
    "        ]\n",
    "        \n",
    "        if len(suspicious) > 0:\n",
    "            print(\"\\n🚨 Potentially suspicious communities (many properties, few violation types):\")\n",
    "            display(suspicious)\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ Community detection requires networkx >= 2.8\")\n",
    "        print(\"   Using simple connected components instead\")\n",
    "        \n",
    "        components = list(nx.connected_components(G))\n",
    "        print(f\"🔗 Found {len(components)} connected components\")\n",
    "        \n",
    "        # Show largest components\n",
    "        components.sort(key=len, reverse=True)\n",
    "        for i, comp in enumerate(components[:5]):\n",
    "            print(f\"   Component {i}: {len(comp)} nodes\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No network available for community detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Fraud Scoring Example\n",
    "\n",
    "Create a simple fraud risk score for properties based on violation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fraud risk scores for properties\n",
    "if 'bin' in [col.lower() for col in df.columns]:\n",
    "    bin_col = [col for col in df.columns if col.lower() == 'bin'][0]\n",
    "    \n",
    "    # Calculate fraud indicators\n",
    "    fraud_scores = []\n",
    "    \n",
    "    for bin_id in df[bin_col].value_counts().head(50).index:  # Top 50 for performance\n",
    "        property_data = df[df[bin_col] == bin_id]\n",
    "        \n",
    "        # Fraud indicators\n",
    "        violation_count = len(property_data)\n",
    "        unique_violation_types = property_data.iloc[:, -1].nunique() if len(property_data.columns) > 1 else 1\n",
    "        \n",
    "        # Simple fraud score (you can make this more sophisticated)\n",
    "        fraud_score = (\n",
    "            violation_count * 0.3 +  # Number of violations\n",
    "            unique_violation_types * 0.7  # Variety of violation types\n",
    "        )\n",
    "        \n",
    "        fraud_scores.append({\n",
    "            'BIN': bin_id,\n",
    "            'Violation_Count': violation_count,\n",
    "            'Unique_Violation_Types': unique_violation_types,\n",
    "            'Fraud_Score': round(fraud_score, 2)\n",
    "        })\n",
    "    \n",
    "    fraud_df = pd.DataFrame(fraud_scores).sort_values('Fraud_Score', ascending=False)\n",
    "    \n",
    "    print(\"🎯 Property Fraud Risk Scores (Top 20):\")\n",
    "    display(fraud_df.head(20))\n",
    "    \n",
    "    # Visualize fraud scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(fraud_df['Violation_Count'], fraud_df['Unique_Violation_Types'], \n",
    "               c=fraud_df['Fraud_Score'], cmap='Reds', alpha=0.7)\n",
    "    plt.colorbar(label='Fraud Score')\n",
    "    plt.xlabel('Number of Violations')\n",
    "    plt.ylabel('Unique Violation Types')\n",
    "    plt.title('Property Fraud Risk Analysis')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"❌ BIN column not found for fraud scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Run Existing Fraud Detection Scripts\n",
    "\n",
    "Execute the pre-built community detection and analysis scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available fraud detection scripts\n",
    "scripts_dir = Path(\"/app/scripts/fraud_detection\")\n",
    "\n",
    "if scripts_dir.exists():\n",
    "    script_files = list(scripts_dir.glob(\"*.py\"))\n",
    "    print(\"🛠️ Available fraud detection scripts:\")\n",
    "    for script in script_files:\n",
    "        print(f\"   • {script.name}\")\n",
    "    \n",
    "    print(\"\\n💡 To run a script, use:\")\n",
    "    print(\"   !python /app/scripts/fraud_detection/SCRIPT_NAME.py\")\n",
    "else:\n",
    "    print(\"❌ Scripts directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the community detection algorithms\n",
    "# Uncomment the line below to execute\n",
    "# !python /app/scripts/fraud_detection/community_detection_algorithms.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 Next Steps\n",
    "\n",
    "Now you can:\n",
    "\n",
    "### 🔄 Explore Different Datasets\n",
    "Change `DATASET_NAME` to explore:\n",
    "- `\"ecb_violations\"` - Environmental violations\n",
    "- `\"maintenance_code_violations\"` - Housing maintenance issues\n",
    "- `\"housing_litigations\"` - Legal cases\n",
    "- `\"job_application_filings\"` - Permit applications\n",
    "- `\"complaints_received\"` - Public complaints\n",
    "\n",
    "### 📊 Advanced Analysis\n",
    "- **Load full datasets**: Remove `nrows=SAMPLE_SIZE`\n",
    "- **Cross-reference datasets**: Join multiple datasets on BIN/address\n",
    "- **Time series analysis**: Look for temporal fraud patterns\n",
    "- **Geographic analysis**: Map fraud hotspots\n",
    "\n",
    "### 🕸️ Network Analysis\n",
    "- **Multi-layer networks**: Connect contractors, properties, inspectors\n",
    "- **Temporal networks**: Track relationships over time\n",
    "- **Anomaly detection**: Find unusual network patterns\n",
    "\n",
    "### 🤖 Machine Learning\n",
    "- **Classification**: Predict fraudulent vs. legitimate activity\n",
    "- **Clustering**: Group similar fraud patterns\n",
    "- **Feature engineering**: Create sophisticated fraud indicators\n",
    "\n",
    "### 🔗 Integration\n",
    "- **Store in Neo4j**: Persist networks for complex queries\n",
    "- **Export results**: Save findings for reporting\n",
    "- **Automate detection**: Schedule regular fraud scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom fraud detection experiments start here! 🚀\n",
    "# \n",
    "# Ideas to try:\n",
    "# 1. Load multiple datasets and join them\n",
    "# 2. Create contractor-property networks\n",
    "# 3. Implement temporal analysis\n",
    "# 4. Build ML models for fraud prediction\n",
    "# 5. Integrate with Neo4j for persistent storage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}