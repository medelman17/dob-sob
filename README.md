# NYC Department of Buildings Data Exploration

A comprehensive data exploration and analysis platform for NYC Department of Buildings datasets, featuring automated data acquisition, graph database storage, and advanced analytics capabilities.

## üèóÔ∏è Project Overview

This project provides tools and infrastructure for:

- **Automated data acquisition** from 18 target NYC DOB public datasets
- **Data quality assessment** and profiling with automated validation
- **Graph database storage** using Neo4j for relationship analysis
- **Pattern matching and anomaly detection** for compliance insights
- **Interactive dashboards** and visualizations for data exploration
- **Machine learning-based risk scoring** for predictive analytics

## üìä Target Datasets

The platform focuses on 18 key NYC Department of Buildings datasets:

**Primary Datasets (Core Building Data):**

1. **Housing Litigations** - Housing litigation cases and status
2. **License Information** - DOB license information and status
3. **Historical Permit Issuance** - Historical DOB permit issuance data
4. **Multiple Dwelling Registrations** - Multiple dwelling registration data
5. **Certificates of Occupancy** - Certificate of occupancy records
6. **Property Data** - Property valuation and assessment data
7. **Job Application Filings** - DOB job application filings
8. **ECB Violations** - Environmental Control Board violations
9. **Active Projects Under Construction** - Currently active construction projects

**Secondary Datasets (Compliance & Enforcement):** 10. **Complaints Received** - DOB complaints received 11. **Maintenance Code Violations** - Housing maintenance code violations 12. **Approved Permits** - Approved DOB permits 13. **DOB Violations** - Department of Buildings violations 14. **Permit Issuance** - DOB permit issuance records 15. **DOB Job Application Filings** - DOB job application filings 16. **Order to Repair/Vacate** - Orders to repair or vacate 17. **Facades Compliance Filings** - Building facades compliance filings 18. **Registration Contacts** - Building registration contact information

## üöÄ Quick Start

### Option 1: Docker Setup (Recommended)

The easiest way to get started is using Docker Compose:

```bash
# Clone the repository
git clone <repository-url>
cd nycdob-data-exploration

# Start all services
docker-compose up -d

# Access the applications:
# - Streamlit Dashboard: http://localhost:8501
# - Neo4j Browser: http://localhost:7474
# - Jupyter Lab: http://localhost:8889
```

### Option 2: Manual Setup

#### Prerequisites

- Python 3.12 or higher
- uv package manager
- Docker and Docker Compose (for Neo4j)

#### Installation Steps

1. **Install Python 3.12+**

   ```bash
   # On macOS with Homebrew
   brew install python@3.12

   # On Ubuntu/Debian
   sudo apt update
   sudo apt install python3.12 python3.12-venv

   # On Windows
   # Download from https://python.org
   ```

2. **Install uv package manager**

   ```bash
   pip install uv
   ```

3. **Set up the project**

   ```bash
   # Clone the repository
   git clone <repository-url>
   cd nycdob-data-exploration

   # Create and activate virtual environment
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate

   # Install dependencies
   uv pip install -r requirements.txt

   # Install development dependencies (optional)
   uv pip install -r requirements-dev.txt
   ```

4. **Start Neo4j database**

   ```bash
   # Using Docker Compose (recommended)
   docker-compose up neo4j -d

   # Or install Neo4j locally following their documentation
   ```

5. **Run the application**

   ```bash
   # Start Streamlit dashboard
   streamlit run main.py

   # Or start Jupyter Lab
   jupyter lab
   ```

## üìÅ Project Structure

```
nycdob-data-exploration/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_acquisition.py    # ‚úÖ NYC DOB data download module
‚îÇ   ‚îú‚îÄ‚îÄ data_profiling/            # Data quality and profiling tools
‚îÇ   ‚îî‚îÄ‚îÄ analysis/                  # Analysis and pattern matching tools
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # ‚úÖ Raw dataset storage (18 subdirectories)
‚îÇ   ‚îî‚îÄ‚îÄ metadata/                  # ‚úÖ Download tracking and metadata
‚îú‚îÄ‚îÄ logs/                          # ‚úÖ Operation logs and monitoring
‚îú‚îÄ‚îÄ docs/                          # Documentation
‚îú‚îÄ‚îÄ .taskmaster/                   # ‚úÖ Task management configuration
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt           # Development dependencies
‚îú‚îÄ‚îÄ pyproject.toml                # Project configuration
‚îú‚îÄ‚îÄ Dockerfile                    # Docker container configuration
‚îú‚îÄ‚îÄ docker-compose.yml            # ‚úÖ Multi-service orchestration
‚îî‚îÄ‚îÄ README.md                     # This file
```

## üì¶ Data Acquisition Module

The project includes a comprehensive data acquisition module (`scripts/etl/data_acquisition.py`) with the following features:

### Key Features

- **Automated Downloads**: Support for all 18 target NYC DOB datasets
- **Multiple Formats**: CSV, JSON, and OData API integration
- **Progress Tracking**: Real-time download progress with speed monitoring
- **Error Handling**: Robust retry logic with exponential backoff
- **Integrity Validation**: MD5 and SHA256 checksum verification
- **Metadata Management**: Download tracking and update detection
- **Batch Operations**: Download primary, secondary, or all datasets

### Usage Examples

```bash
# Download a specific dataset
python scripts/etl/data_acquisition.py --dataset housing_litigations

# Download all primary datasets (first 9)
python scripts/etl/data_acquisition.py --primary

# Download all secondary datasets (remaining 9)
python scripts/etl/data_acquisition.py --secondary

# Download all 18 datasets
python scripts/etl/data_acquisition.py --all

# Check for dataset updates
python scripts/etl/data_acquisition.py --check-updates

# View download summary
python scripts/etl/data_acquisition.py --summary

# Force re-download existing files
python scripts/etl/data_acquisition.py --all --force
```

### Download Statistics

Successfully tested with:

- **Housing Litigations**: 42.5 MB (44,523,339 bytes)
- **Certificates of Occupancy**: 27.3 MB (28,596,373 bytes)
- **Progress Tracking**: Real-time speed monitoring (e.g., 8.5 MB/s)
- **Integrity Verification**: MD5 checksum validation for all downloads

## üîß Configuration

### Environment Variables

Create a `.env` file in the project root:

```bash
# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# Data Sources
NYC_DOB_API_KEY=your_api_key_here  # If required

# Application Settings
STREAMLIT_SERVER_PORT=8501
JUPYTER_PORT=8888

# Data Acquisition Settings
DATA_ACQUISITION_LOG_LEVEL=INFO
REQUEST_TIMEOUT=300
MAX_RETRIES=3
```

### Neo4j Setup

The default configuration uses:

- **Browser Interface**: http://localhost:7474
- **Bolt Protocol**: bolt://localhost:7687
- **Username**: neo4j
- **Password**: password

## üê≥ Docker Services

The `docker-compose.yml` defines three services:

### Application Service (`app`)

- **Port**: 8501
- **Purpose**: Streamlit dashboard
- **Volumes**: Code and data directories mounted for development

### Neo4j Database (`neo4j`)

- **Ports**: 7474 (browser), 7687 (bolt)
- **Version**: Neo4j 5.15 Community Edition
- **Plugins**: APOC procedures enabled
- **Memory**: 2GB heap, 1GB page cache

### Jupyter Service (`jupyter`)

- **Port**: 8889 (mapped to container port 8888)
- **Purpose**: Interactive data analysis
- **Access**: http://localhost:8889 (no token required)

## üìä Key Dependencies

### Core Libraries

- **pandas** (‚â•2.0.0): Data manipulation and analysis
- **polars** (‚â•0.20.0): High-performance DataFrame library
- **neo4j** (‚â•5.8.0): Neo4j database driver
- **networkx** (‚â•3.1): Graph analysis and algorithms
- **requests** (‚â•2.31.0): HTTP client for data acquisition

### Visualization

- **plotly** (‚â•5.15.0): Interactive plotting
- **streamlit** (‚â•1.28.0): Web application framework
- **matplotlib** (‚â•3.7.0): Static plotting
- **seaborn** (‚â•0.12.0): Statistical visualization

### Machine Learning

- **scikit-learn** (‚â•1.3.0): Machine learning algorithms
- **numpy** (‚â•1.24.0): Numerical computing

### Development Tools

- **pytest** (‚â•7.4.0): Testing framework
- **black** (‚â•23.7.0): Code formatting
- **isort** (‚â•5.12.0): Import sorting
- **mypy** (‚â•1.5.0): Type checking

## ‚úÖ Implementation Status

### Completed Tasks

- ‚úÖ **Task 1**: Setup Development Environment
- ‚úÖ **Task 2**: Docker Compose Configuration
- ‚úÖ **Task 3.1**: Create Data Acquisition Module Structure
- ‚úÖ **Task 3.2**: Implement Primary Dataset Download Functions

### Current Progress

- **Data Acquisition**: Production-ready module with comprehensive CLI
- **Infrastructure**: Docker services configured and tested
- **Directory Structure**: Complete with organized data storage
- **Logging**: Centralized logging with daily log files
- **Metadata Tracking**: Download tracking and update detection

### Next Steps

- **Task 3.3**: Implement Secondary Dataset Download Functions
- **Task 4**: Database Schema Design and Implementation
- **Task 5**: ETL Pipeline Development
- **Task 6**: Data Quality Assessment Tools

## üîç Troubleshooting

### Common Issues

1. **Port conflicts**

   ```bash
   # Check what's using the ports
   lsof -i :8501  # Streamlit
   lsof -i :7474  # Neo4j browser
   lsof -i :7687  # Neo4j bolt
   ```

2. **Docker permission issues**

   ```bash
   # Add user to docker group (Linux)
   sudo usermod -aG docker $USER
   # Log out and back in
   ```

3. **Virtual environment issues**

   ```bash
   # Remove and recreate virtual environment
   rm -rf .venv
   uv venv
   source .venv/bin/activate
   uv pip install -r requirements.txt
   ```

4. **Neo4j connection issues**

   ```bash
   # Check Neo4j logs
   docker-compose logs neo4j

   # Restart Neo4j service
   docker-compose restart neo4j
   ```

5. **Data acquisition issues**

   ```bash
   # Check download logs
   cat logs/data_acquisition_$(date +%Y%m%d).log

   # Test with a small dataset first
   python scripts/etl/data_acquisition.py --dataset housing_litigations

   # Force re-download if needed
   python scripts/etl/data_acquisition.py --dataset housing_litigations --force
   ```

### Performance Optimization

- **Memory**: Adjust Neo4j memory settings in `docker-compose.yml` based on available RAM
- **Storage**: Use SSD storage for better I/O performance
- **Network**: Ensure Docker has sufficient resources allocated
- **Downloads**: Use batch downloads during off-peak hours for large datasets

## üß™ Testing

Run the test suite:

```bash
# Install development dependencies
uv pip install -r requirements-dev.txt

# Run tests
pytest

# Run with coverage
pytest --cov=scripts

# Type checking
mypy scripts/

# Code formatting
black scripts/
isort scripts/

# Test data acquisition module
python scripts/etl/data_acquisition.py --summary
```

## üìö Next Steps

After completing the environment setup:

1. **Data Acquisition**: ‚úÖ Complete - Set up data download scripts
2. **Database Schema**: Design and implement Neo4j schema
3. **ETL Pipeline**: Build data transformation pipelines
4. **Analysis Tools**: Implement pattern matching and anomaly detection
5. **Dashboard**: Create interactive visualizations

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and linting
5. Submit a pull request

## üìÑ License

[Add your license information here]

## üìû Support

For questions or issues:

- Create an issue in the repository
- Check the troubleshooting section above
- Review the project documentation in `/docs/`
- Check the logs in `/logs/` for detailed error information

---

**Note**: This project is actively under development. The data acquisition module is production-ready and successfully tested with NYC Open Data endpoints. All 18 target datasets are configured and ready for download.
